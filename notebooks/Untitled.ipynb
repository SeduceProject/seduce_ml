{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: texttable in /usr/local/lib/python3.7/site-packages (1.6.2)\r\n",
      "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/site-packages (0.0)\r\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/site-packages (from sklearn) (0.21.2)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/site-packages (from scikit-learn->sklearn) (0.13.2)\r\n",
      "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.3.0)\r\n",
      "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.17.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip3 install texttable sklearn\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from texttable import Texttable\n",
    "from sklearn.externals import joblib\n",
    "from numpy import nan as nan\n",
    "\n",
    "import numpy as np\n",
    "import requests\n",
    "from functional import seq\n",
    "import os\n",
    "import json\n",
    "import calendar\n",
    "import time\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import uuid\n",
    "\n",
    "NORMALIZATION_SERVER = 200\n",
    "# NORMALIZATION_COOLING = 20000\n",
    "ADD_EXTERNAL_TEMPERATURE = False\n",
    "\n",
    "\n",
    "def simulate_consumption_function(data):\n",
    "\n",
    "    weight = [\n",
    "        0.82, 0.85, 0.87, 0.90, 0.97, 0.99, 0.93, 0.82, 0.72, 0.70, 0.85, 0.98,\n",
    "        0.82, 0.85, 0.87, 0.90, 0.97, 0.99, 0.93, 0.82, 0.72, 0.70, 0.78, 0.85,\n",
    "        0.95, 0.94, 0.87, 0.90, 0.97, 0.99, 0.93, 0.82, 0.72, 0.70, 0.70, 0.68,\n",
    "        0.97, 0.99, 0.87, 0.90, 0.97, 0.99, 0.93, 0.82, 0.72, 0.70, 0.70, 0.68,\n",
    "    ]\n",
    "\n",
    "    if len(data) < 10:\n",
    "        raise Exception(\"Error: len(data) < 0\")\n",
    "\n",
    "    consumption = sum([x * y for (x, y) in zip(weight, data)])\n",
    "    normalized_consumption = consumption / len(weight)\n",
    "\n",
    "    # return math.sin(normalized_consumption)\n",
    "    return normalized_consumption\n",
    "\n",
    "\n",
    "def generate_fake_consumption_data(nb_servers, nb_data):\n",
    "    x = np.random.random((nb_data, nb_servers))\n",
    "    y = np.apply_along_axis(simulate_consumption_function, 1, x)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def cluster_average_temperature(start_epoch, end_epoch, side=\"back\"):\n",
    "\n",
    "    resp = requests.get(\"https://api.seduce.fr/infrastructure/description/tree\")\n",
    "    resp_json = resp.json()\n",
    "    servers_names = list(resp_json.get(\"power\").keys())\n",
    "\n",
    "    back_temperature_sensors = [v.get(side) for v in resp_json.get(\"temperature\").values() if v.get(side) is not None]\n",
    "\n",
    "    server_sensors_list = []\n",
    "\n",
    "    for back_temperature_sensor in back_temperature_sensors:\n",
    "        sensors = [(s.get(\"tags\")[0], s.get(\"serie\")) for s in back_temperature_sensor.values() if s.get(\"tags\")]\n",
    "\n",
    "        server_sensors_list += sensors\n",
    "\n",
    "    sorted_server_sensors_list = sorted(server_sensors_list, key=lambda x: int(x[0].split(\"-\")[-1]))\n",
    "    sorted_sensors = [x[1] for x in sorted_server_sensors_list]\n",
    "\n",
    "    temperatures = []\n",
    "\n",
    "    for sensor in sorted_sensors:\n",
    "        temperature_url = \"https://api.seduce.fr/sensors/%s/measurements?start_date=%s&end_date=%s\" % (sensor, int(start_epoch), int(end_epoch))\n",
    "        sensor_temperatures = requests.get(temperature_url).json().get(\"values\")\n",
    "        temperatures += [np.mean(sensor_temperatures)]\n",
    "        print(\".\", end=\"\")\n",
    "    print(\"\")\n",
    "\n",
    "    return temperatures\n",
    "\n",
    "\n",
    "def average_temperature_aggregated_by_minute(start_epoch, end_epoch, side=\"back\"):\n",
    "    temperature_url = \"https://dashboard.seduce.fr/rack/%s/temperatures/aggregated?start_date=%ss&end_date=%ss\" % (side, int(start_epoch), int(end_epoch))\n",
    "    sensor_temperatures = requests.get(temperature_url).json()\n",
    "\n",
    "    return sensor_temperatures\n",
    "\n",
    "\n",
    "def generate_real_consumption_data(start_date=None,\n",
    "                                   end_date=None,\n",
    "                                   show_progress=True,\n",
    "                                   data_file_path=\"data.json\",\n",
    "                                   group_by=60,\n",
    "                                   scaler=None):\n",
    "\n",
    "    if start_date is None:\n",
    "        # start_date = \"2019-05-24T08:00:00.000Z\"\n",
    "        start_date = \"2019-06-01T06:00:00.000Z\"\n",
    "        # start_date = \"2019-07-07T06:00:00.000Z\"\n",
    "    if end_date is None:\n",
    "        # end_date = \"2019-06-11T08:00:00.000Z\"\n",
    "        end_date = \"2019-07-08T09:09:35.000Z\"\n",
    "\n",
    "    # Group node data every 120 minutes\n",
    "    # group_by = 60\n",
    "    # group_by = 35\n",
    "    # group_by = 2 * 60\n",
    "\n",
    "    seduce_infrastructure_tree = requests.get(\"https://api.seduce.fr/infrastructure/description/tree\").json()\n",
    "\n",
    "    power_infrastructure_tree = requests.get(\"https://api.seduce.fr/power_infrastructure/description/tree\").json()\n",
    "    # servers_names_raw = power_infrastructure_tree['children'][0]['children'][1]['children'][1]['node'].get(\"children\")\n",
    "    # servers_names_raw = [f\"ecotype-{i}\" for i in range(25, 37)]\n",
    "    servers_names_raw = [f\"ecotype-{i}\" for i in range(37, 49)]\n",
    "    # servers_names_raw = sorted(servers_names_raw, key=lambda x: int(x.split(\"-\")[1]))\n",
    "\n",
    "    servers_names = seq(servers_names_raw)\n",
    "    # servers_names = servers_names.take(2)\n",
    "\n",
    "    # nodes_names = servers_names[:1] + [\"back_temperature\"]\n",
    "    # nodes_names = servers_names\n",
    "\n",
    "    reload_data = False\n",
    "    # reload_data = True\n",
    "    if not os.path.exists(data_file_path):\n",
    "        reload_data = True\n",
    "    else:\n",
    "        with open(data_file_path, \"r\") as data_file:\n",
    "            data = json.load(data_file)\n",
    "            if data.get(\"start_date\") != start_date or data.get(\"end_date\") != end_date or data.get(\"group_by\") != group_by:\n",
    "                reload_data = True\n",
    "\n",
    "    if reload_data:\n",
    "\n",
    "        data = {\n",
    "            \"consumptions\": {},\n",
    "            \"start_date\": start_date,\n",
    "            \"end_date\": end_date,\n",
    "            \"group_by\": group_by,\n",
    "            \"room_temperature\": [],\n",
    "            \"min_consumption\": None,\n",
    "            \"max_consumption\": None,\n",
    "            \"min_temperature\": None,\n",
    "            \"max_temperature\": None\n",
    "        }\n",
    "\n",
    "        epoch_times = [calendar.timegm(time.strptime(t, '%Y-%m-%dT%H:%M:%S.000Z')) for t in [start_date, end_date]]\n",
    "        start_epoch = min(epoch_times)\n",
    "        end_epoch = max(epoch_times)\n",
    "\n",
    "        group_by_seconds = \"%ss\" % (group_by * 60)\n",
    "        dump_data_url = \"\"\"https://dashboard.seduce.fr/dump/all/aggregated?start_date=%ss&end_date=%ss&group_by=%s\"\"\" % (start_epoch, end_epoch, group_by_seconds)\n",
    "        dump_data = requests.get(dump_data_url).json()\n",
    "\n",
    "        incomplete_series = [v\n",
    "                             for v in dump_data[\"sensors_data\"].values()\n",
    "                             if len(v.get(\"timestamps\", [])) != max([len(v1.get(\"timestamps\"))\n",
    "                                                                     for v1 in dump_data[\"sensors_data\"].values()\n",
    "                                                                     ])\n",
    "                             ]\n",
    "\n",
    "        if len(incomplete_series) > 0:\n",
    "            raise(\"Some series are incomplete!\")\n",
    "\n",
    "        data[\"timestamps\"] = list(set([ts\n",
    "                                       for v in dump_data[\"sensors_data\"].values()\n",
    "                                       for ts in v.get(\"timestamps\")\n",
    "                                       ]))\n",
    "\n",
    "        data[\"sensors_data\"] = dump_data[\"sensors_data\"]\n",
    "\n",
    "        # get consumptions of servers\n",
    "        for server_name in servers_names:\n",
    "\n",
    "            # find PDUS\n",
    "            node_pdus = seduce_infrastructure_tree[\"power\"][server_name].values()\n",
    "\n",
    "            # find back_temperature_sensors\n",
    "            back_temperature_sensor = [sensor\n",
    "                                       for x in seduce_infrastructure_tree[\"temperature\"].values()\n",
    "                                       for (side, sensor_bus) in x.items()\n",
    "                                       if side == \"back\"\n",
    "                                       for sensor in sensor_bus.values()\n",
    "                                       if server_name in sensor.get(\"tags\", [])\n",
    "                                       ][0].get(\"serie\")\n",
    "\n",
    "            # Create a dict with all the infos related to the node\n",
    "            data[\"consumptions\"][server_name] = {\n",
    "                \"means\": [sum(tuple_n) if None not in tuple_n else -1\n",
    "                                       for tuple_n in zip(*[v.get(\"means\")\n",
    "                                                            for k,v in dump_data.get(\"sensors_data\").items()\n",
    "                                                            if k in node_pdus\n",
    "                                                            ])\n",
    "                                       ],\n",
    "                # \"temperatures\": dump_data.get(\"sensors_data\")[back_temperature_sensor][\"maxs\"],\n",
    "                \"temperatures\": dump_data.get(\"sensors_data\")[back_temperature_sensor][\"means\"],\n",
    "                \"timestamps\": dump_data.get(\"sensors_data\")[back_temperature_sensor][\"timestamps\"],\n",
    "            }\n",
    "\n",
    "            if show_progress:\n",
    "                print('.', end='')\n",
    "\n",
    "        data[\"room_temperature\"] = data.get(\"sensors_data\")[\"28b8fb2909000003\"][\"means\"]\n",
    "\n",
    "        # Detect incomplete data\n",
    "        filter_timestamps = []\n",
    "        for server_name in servers_names:\n",
    "            data_server = data[\"consumptions\"][server_name]\n",
    "\n",
    "            ziped_big_array = zip(data_server[\"timestamps\"], data_server[\"means\"], data_server[\"temperatures\"])\n",
    "            filtered_ziped_big_array = [tuple_n\n",
    "                                        for tuple_n in ziped_big_array\n",
    "                                        if -1 in [tuple_n[1], tuple_n[2]] or None in [tuple_n[1], tuple_n[2]]\n",
    "                                        ]\n",
    "            if len(filtered_ziped_big_array) > 0:\n",
    "                filter_timestamps += [x[0] for x in filtered_ziped_big_array]\n",
    "\n",
    "        # Filter incomplete data\n",
    "        for server_name in servers_names:\n",
    "            data_server = data[\"consumptions\"][server_name]\n",
    "\n",
    "            ziped_big_array = zip(data_server[\"timestamps\"], data_server[\"means\"], data_server[\"temperatures\"])\n",
    "            filtered_ziped_big_array = [tuple_n\n",
    "                                        for tuple_n in ziped_big_array\n",
    "                                        if tuple_n[0] not in filter_timestamps\n",
    "                                        ]\n",
    "\n",
    "            data_server[\"timestamps\"] = [tuple_n[0] for tuple_n in filtered_ziped_big_array]\n",
    "            data_server[\"means\"] = [tuple_n[1] for tuple_n in filtered_ziped_big_array]\n",
    "            data_server[\"temperatures\"] = [tuple_n[2] for tuple_n in filtered_ziped_big_array]\n",
    "\n",
    "        # Normalize data\n",
    "        for server_name in servers_names:\n",
    "            data_server = data[\"consumptions\"][server_name]\n",
    "\n",
    "            data_server[\"means\"] = [x\n",
    "                                    for x in data_server[\"means\"]]\n",
    "            data_server[\"temperatures\"] = [x\n",
    "                                           for x in data_server[\"temperatures\"]]\n",
    "\n",
    "        data[\"room_temperature\"] = [tuple_2[1]\n",
    "                                    for tuple_2 in zip(data[\"timestamps\"], data[\"room_temperature\"])\n",
    "                                    if tuple_2[0] not in filter_timestamps]\n",
    "\n",
    "        with open(data_file_path, \"w+\") as data_file:\n",
    "            json.dump(data, data_file)\n",
    "    else:\n",
    "        with open(data_file_path, \"r\") as data_file:\n",
    "            data = json.load(data_file)\n",
    "\n",
    "    timestamps_with_all_data = data[\"timestamps\"]\n",
    "\n",
    "    visualization_data = servers_names \\\n",
    "        .map(lambda x: data.get(\"consumptions\")[x]) \\\n",
    "        .map(lambda c: zip(c[\"timestamps\"], c[\"means\"], c[\"temperatures\"])) \\\n",
    "        .map(lambda z: seq(z)\n",
    "             .filter(lambda z: z[0] in timestamps_with_all_data))\n",
    "\n",
    "    v = visualization_data.map(lambda z: seq(z)\n",
    "                               .map(lambda z: z[1])\n",
    "                               .map(lambda x: x if x is not None else 0))\n",
    "\n",
    "    temperatures = data[\"room_temperature\"]\n",
    "\n",
    "    x = np.transpose(np.array(\n",
    "        v.take(servers_names.size()).map(lambda x: x.map(lambda z: z).to_list()).to_list()))\n",
    "\n",
    "    # Compute values that will be predicted\n",
    "    def select_tuple_n(tuple_n):\n",
    "        return list(tuple_n)\n",
    "        # return [tuple_n[23]]\n",
    "        # return tuple_n[0]\n",
    "        # return max(tuple_n)\n",
    "        # return 30000\n",
    "        # return sum(tuple_n)\n",
    "\n",
    "    raw_values_that_will_be_predicted = [select_tuple_n(tuple_n)\n",
    "                                         for tuple_n in zip(*[data.get(\"consumptions\")[server_name][\"temperatures\"]\n",
    "                                                              # for server_name, server_data in data.get(\"consumptions\").items()\n",
    "                                                              # if server_name in servers_names_raw\n",
    "                                                              for server_name in servers_names\n",
    "                                                              ]\n",
    "                                                            )\n",
    "                                         ]\n",
    "\n",
    "    y = np.array(raw_values_that_will_be_predicted)\n",
    "\n",
    "    timestamps_labels = timestamps_with_all_data\n",
    "\n",
    "    if ADD_EXTERNAL_TEMPERATURE:\n",
    "        # Add external temperature to the the 'x' array\n",
    "        z = np.array(seq(temperatures).map(lambda x: x if x is not None else 0).to_list()).reshape(len(x), 1)\n",
    "        x = np.append(x, z, axis=1)\n",
    "\n",
    "    # Scale values\n",
    "    if scaler is None:\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "    all_non_scaled_values = np.copy(x)\n",
    "    all_non_scaled_values = np.append(all_non_scaled_values, y, axis=1)\n",
    "    scaled_values = scaler.fit_transform(all_non_scaled_values)\n",
    "\n",
    "    scaled_x, scaled_y = scaled_values[:, :len(servers_names_raw)], scaled_values[:, -len(servers_names_raw):]\n",
    "\n",
    "    return scaled_x, scaled_y, timestamps_labels, data, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-390b2413647a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mCOMPARISON_PLOT_DATA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'nb_layers'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'neurons_per_layers'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'activation_function'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mse'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m3.6156303271737893\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mse_perc_95'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m29.844275604199883\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rmse'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1.3764298699455324\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rmse_perc_95'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m5.3545053547850285\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dump_path'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'data/seduceml_2019_07_24_T_13_09_32.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tmp_figures_folder'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'tmp/2019_07_24__13_08_03'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'scaler_path'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'data/scaler_9c0463ac-6557-4bcf-80db-49c49254cc52'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../data.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdata_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mtraining_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data.json'"
     ]
    }
   ],
   "source": [
    "# NETWORK_PATH = 'data/seduceml_2019_07_17_T_14_37_00.h5'\n",
    "# NETWORK_PATH = 'data/seduceml_2019_07_18_T_15_48_18.h5'\n",
    "# NETWORK_PATH = 'data/seduceml_2019_07_18_T_15_00_12.h5'\n",
    "# NETWORK_PATH = 'data/seduceml_2019_07_18_T_16_04_23.h5'\n",
    "# NETWORK_PATH = 'data/seduceml_2019_07_18_T_16_30_59.h5'\n",
    "NETWORK_PATH = \"last\"\n",
    "\n",
    "start_date = \"2019-07-10T00:00:00.000Z\"\n",
    "end_date = \"2019-07-24T15:00:00.000Z\"\n",
    "GROUP_BY = 60\n",
    "\n",
    "COMPARISON_PLOT_DATA = [{'epoch': 200, 'nb_layers': 3, 'neurons_per_layers': 64, 'activation_function': 'relu', 'mse': 3.6156303271737893, 'mse_perc_95': 29.844275604199883, 'rmse': 1.3764298699455324, 'rmse_perc_95': 5.3545053547850285, 'dump_path': 'data/seduceml_2019_07_24_T_13_09_32.h5', 'tmp_figures_folder': 'tmp/2019_07_24__13_08_03', 'scaler_path': 'data/scaler_9c0463ac-6557-4bcf-80db-49c49254cc52'}]\n",
    "\n",
    "with open(\"../data.json\", \"r\") as data_file:\n",
    "    training_data = json.load(data_file)\n",
    "\n",
    "EPOCHS = list(set([d[\"epoch\"] for d in COMPARISON_PLOT_DATA]))\n",
    "NB_LAYERS = list(set([d[\"nb_layers\"] for d in COMPARISON_PLOT_DATA]))\n",
    "NEURONS_PER_LAYER = list(set([d[\"neurons_per_layers\"] for d in COMPARISON_PLOT_DATA]))\n",
    "ACTIVATION_FUNCTIONS = list(set([d[\"activation_function\"] for d in COMPARISON_PLOT_DATA]))\n",
    "\n",
    "for PLOT_DATA in COMPARISON_PLOT_DATA:\n",
    "\n",
    "    if NETWORK_PATH == \"last\":\n",
    "        netwok_path = \"../data/%s\" % (sorted(os.listdir(\"../data\"))[-1])\n",
    "    else:\n",
    "        netwok_path = NETWORK_PATH\n",
    "\n",
    "    oracle = load_model(\"../\"+PLOT_DATA[\"dump_path\"])\n",
    "\n",
    "    scaler = joblib.load(\"../\"+PLOT_DATA[\"scaler_path\"])\n",
    "\n",
    "    x, y, tss, data, scaler = generate_real_consumption_data(start_date,\n",
    "                                                             end_date,\n",
    "                                                             data_file_path=\"data_validation.json\",\n",
    "                                                             group_by=GROUP_BY,\n",
    "                                                             scaler=scaler)\n",
    "\n",
    "    start_step = 0\n",
    "    end_step = len(y)\n",
    "\n",
    "    plot_data = []\n",
    "\n",
    "    # server_id = 32\n",
    "    server_id = 6\n",
    "    \n",
    "    neighbours = [i for i in range(server_id - 1, 12)]\n",
    "    neighbours = [server_id - 2, server_id - 1, server_id + 1, server_id + 2, server_id + 3]\n",
    "    neighbours_weights = [0 if i == server_id else 1 for i in range(0, 12)]\n",
    "    \n",
    "    print(neighbours)\n",
    "    print(neighbours_weights)\n",
    "\n",
    "    for idx in range(0, len(y)):\n",
    "        test_input = np.array([x[idx]])\n",
    "        expected_value = y[idx]\n",
    "        result = oracle.predict(test_input)[0]\n",
    "\n",
    "        unscaled_expected_values = scaler.inverse_transform(np.array([np.append(np.copy(test_input), expected_value)]))\n",
    "        unscaled_predicted_values = scaler.inverse_transform(np.array([np.append(np.copy(test_input), result)]))\n",
    "\n",
    "        expected_temp = unscaled_expected_values[:, -len(expected_value):][0]\n",
    "        predicted_temp = unscaled_predicted_values[:, -len(expected_value):][0]\n",
    "\n",
    "        mse = ((predicted_temp - expected_temp) ** 2)\n",
    "\n",
    "        plot_data += [{\n",
    "            \"x\": idx,\n",
    "            \"y_actual\": expected_value,\n",
    "            \"y_pred\": result,\n",
    "            \"temp_room\": data.get(\"room_temperature\")[idx],\n",
    "            \"mse_mean\": mse.mean(axis=0),\n",
    "            \"mse_raw\": mse,\n",
    "            \"rmse_mean\": np.sqrt(mse).mean(axis=0),\n",
    "            \"rmse_raw\": np.sqrt(mse),\n",
    "            \"temp_actual\": expected_temp,\n",
    "            \"temp_pred\": predicted_temp,\n",
    "            \"server_power_consumption\": unscaled_predicted_values[0][server_id],\n",
    "            \"server_power_consumption_neighbours\": np.average(unscaled_predicted_values[0][neighbours], weights=range(0, len(neighbours)))\n",
    "        }]\n",
    "\n",
    "    # MSE\n",
    "    flatten_mse = np.array([d[\"mse_raw\"] for d in plot_data]).flatten()\n",
    "    mse = flatten_mse.mean()\n",
    "    mse_perc_95 = flatten_mse[flatten_mse > np.percentile(flatten_mse, 95)].mean()\n",
    "\n",
    "    # RMSE\n",
    "    flatten_rmse = np.array([d[\"rmse_raw\"] for d in plot_data]).flatten()\n",
    "    rmse = flatten_rmse.mean()\n",
    "    rmse_perc_95 = flatten_rmse[flatten_rmse > np.percentile(flatten_rmse, 95)].mean()\n",
    "\n",
    "    print(\"best_mse: %s\" % (mse))\n",
    "    print(\"best_mse_perc_95: %s\" % (mse_perc_95))\n",
    "    print(\"best_rmse: %s\" % (rmse))\n",
    "    print(\"best_rmse_perc_95: %s\" % (rmse_perc_95))\n",
    "\n",
    "    # date_str = time.strftime(\"%Y_%m_%d_T_%H_%M_%S\", time.localtime(epoch))\n",
    "    # neural_net_dump_path = \"data/seduceml_%s.h5\" % date_str\n",
    "\n",
    "    PLOT_DATA[\"mse\"] = mse\n",
    "    PLOT_DATA[\"mse_perc_95\"] = mse_perc_95\n",
    "    PLOT_DATA[\"rmse\"] = rmse\n",
    "    PLOT_DATA[\"rmse_perc_95\"] = rmse_perc_95\n",
    "\n",
    "    sorted_plot_data = sorted(plot_data, key=lambda d: d[\"x\"])[start_step:end_step]\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = plt.axes()\n",
    "\n",
    "    x_data = [d[\"x\"] for d in sorted_plot_data]\n",
    "    y1_data = [d[\"temp_actual\"][server_id] for d in sorted_plot_data]\n",
    "    y2_data = [d[\"temp_pred\"][server_id] for d in sorted_plot_data]\n",
    "    y3_data = [d[\"temp_room\"] for d in sorted_plot_data]\n",
    "    y4_data = [d[\"server_power_consumption\"] for d in sorted_plot_data]\n",
    "    y5_data = [d[\"server_power_consumption_neighbours\"] for d in sorted_plot_data]\n",
    "    x_data = range(0, len(y1_data))\n",
    "    \n",
    "    x_np = np.array(x_data[1:])\n",
    "    y3_np = np.array(y3_data[1:])\n",
    "    y1_np = np.array(y1_data[1:]) - y3_np\n",
    "    y2_np = np.array(y2_data[1:]) - y3_np\n",
    "    y4_np = np.array(y4_data[1:])\n",
    "    y5_np = np.array(y5_data[1:])\n",
    "\n",
    "    ax.plot(x_np, y1_np, color='blue', label='actual temp.')\n",
    "    ax.plot(x_np, y2_np, color='red', label='predicted temp.', alpha=0.5)\n",
    "    #ax.plot(x_np, y1_np - y2_np, color='red', label='diff temp.', alpha=0.5)\n",
    "    # ax.plot(x_data, y3_data, color='green', label='room temp.', alpha=0.5)\n",
    "\n",
    "    ax2 = ax.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "    ax2.plot(x_np, y3_np, color='green', label='outisde temp.', alpha=0.5)\n",
    "    #ax2.plot(x_np, y4_np, color='green', label='power consumption.', alpha=0.5)\n",
    "    #ax2.plot(x_np, y5_np, color='orange', label='power consumption. neighbours', alpha=0.5)\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "    plt.xlabel('Time (hour)')\n",
    "    plt.ylabel('Back temperature of ecotype-%s (deg. C)' % (server_id+1))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "import inspect\n",
    "\n",
    "def func(X, a, b, c):\n",
    "    x1, x2 = X\n",
    "    return a + b * x1 + c * (x2 - x1) * x1\n",
    "    # return a + b * np.log(c * x)\n",
    "\n",
    "# cm = plt.cm.get_cmap('RdYlBu')\n",
    "cm = plt.cm.get_cmap('hot_r')\n",
    "nb_args = len(inspect.getargspec(func).args)\n",
    "p0 = [0 for i in range(1, nb_args)]\n",
    "popt, pcov = curve_fit(func, (y4_np, y5_np), y1_np, p0)\n",
    "\n",
    "print(popt)\n",
    "print(pcov)\n",
    "\n",
    "#p0 = np.array([23.62261268, -0.13201981, 0.51935314, 0.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "\n",
    "x_data = [d[\"x\"] for d in sorted_plot_data]\n",
    "y1_data = [d[\"temp_actual\"][server_id] for d in sorted_plot_data]\n",
    "\n",
    "y_pred = [func(y, *popt) for y in zip(y4_np, y5_np)]\n",
    "y_pred_np = np.array(y_pred)\n",
    "\n",
    "p1 = ax.plot(x_data[1:], y1_np, color='blue', label='actual temp.')\n",
    "p2 = ax.plot(x_data[1:], y_pred, color='red', label='predicted temp.')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('Time (hour)')\n",
    "plt.ylabel('Back temperature of ecotype-%s (deg. C)' % (server_id+1))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
